import os
import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.chains import create_extraction_chain, ConversationalRetrievalChain
from langchain.document_loaders.csv_loader import CSVLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS

from langchain.llms import OpenAI
from langchain.chains import LLMRequestsChain, LLMChain
from langchain.prompts import PromptTemplate

from langchain.memory import ConversationBufferMemory
request_memory = ConversationBufferMemory(input_key='query', memory_key='chat_history')

http_url = "http://127.0.0.1:8080/"
os.environ['OPENAI_API_KEY'] = "apií‚¤ ì…ë ¥"
base_llm = ChatOpenAI(temperature=0.0, model_name="gpt-3.5-turbo-0613")

## í™”ë©´ì— í‘œì‹œë˜ëŠ” íƒ€ì´í‹€ê³¼ í…ìŠ¤íŠ¸í•„ë“œ
st.title('ğŸ¦œğŸ”— Langchain chatbot (csv file)')
question = st.text_input('ì§ˆë¬¸ ë‚´ìš©ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.(ë‚ ì§œëŠ” ìµœëŒ€í•œ ëª…í™•í•˜ê²Œ)') 

########### CSV ë°ì´í„° ë¡œë“œ ë° embeddingí•˜ì—¬ vectorstore ì €ì¥ ë° Chain ì¤€ë¹„ ###########

loader = CSVLoader(file_path='./langchain_chatbot_source.csv')

data = loader.load()

embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(data, embeddings)

conversation_chain = ConversationalRetrievalChain.from_llm(
llm = ChatOpenAI(temperature=0.0,model_name='gpt-3.5-turbo-0613'),
retriever=vectorstore.as_retriever(), return_source_documents=True, verbose=True)

################################ CSV ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ ################################


######### vectorstore ì €ì¥ì†Œì—ì„œ ìœ ì € ì§ˆë¬¸ì— ê°€ê¹Œìš´ ë‚´ìš©ì„ ë½‘ì•„ ê° CSV ì»¬ëŸ¼ì˜ ê°’ì„ ì¶”ì¶œ #########

def conversational_chat(query)  -> list:
      
      chat_history = []
      result = conversation_chain({"question": query, "chat_history": chat_history})
      
      return result["source_documents"][0].page_content.split("\n")

def get_csv_extraction(question: str) -> dict:
      conversation_response = conversational_chat(question)
      with st.expander('CSV Extraction History'): 
        st.info(conversation_response)

      csv_schema = {
          "properties": {
              "title" : {"type": "string"},
              "heading" : {"type": "string"},
              "content" : {"type": "string"},
              "url" : {"type": "string"},
              "param" : {"type": "string"},
          }
      }

      csv_extraction_chain = create_extraction_chain(csv_schema, base_llm)

      csv_extraction_response = csv_extraction_chain.run(conversation_response)

      return csv_extraction_response[0]

############################### CSV ì»¬ëŸ¼ì˜ ê°’ ì¶”ì¶œ ì™„ë£Œ ###############################


##### ì§ˆë¬¸ê³¼ ì§ˆë¬¸ì˜ paramì„ ì¶”ì¶˜í•œ dictì„ ì´ìš©í•´ HTTP GETìš”ì²­í•˜ì—¬ ë°ì´í„°ë¥¼ ë°›ì€ í›„ GPTë‹µë³€ ë°˜í™˜ ####

def get_request_get_answer(question: str, url: str, params: str, is_contain_param = False) -> str:
           
      template = """ You are a PRIVATE chatbot, only answer the question by using the provided Between >>> and <<<.
      Extract the answer to the question '{query}' or say "not found" if the information is not contained.
      Always return in Korean.
      Use the format
      Extracted:<answer or "not data">
      >>> {requests_result} <<<
      Extracted:"""

      PROMPT = PromptTemplate(
          input_variables=["query", "requests_result"],
          template=template,
      )

      chain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=0.0, model_name="gpt-4"), prompt=PROMPT, memory=request_memory, verbose=True))
      if is_contain_param:
            inputs = {
               "query": question,
               "url": http_url + url + params,
            }
      else:
            inputs = {
               "query": question,
               "url": http_url + url,
            }

      response = chain(inputs)

      with st.expander('Request History'):
         st.info(response)

      return response["output"]

#################################### ë‹µë³€ ë°˜í™˜ í•¨ìˆ˜ ####################################


################ ì¶”ì¶œ ê°’ì—ì„œ paramì˜ ê°’ì´ ìˆìœ¼ë©´ ìœ ì €ì˜ ì§ˆë¬¸ì—ì„œ í•´ë‹¹ ê°’ì„ ì¶”ì¶œ  ################
################ paramì˜ key ê°’ì— ëŒ€í•œ ë‚´ìš©ì´ ì§ˆë¬¸ì— ì—†ìœ¼ë©´ ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ë°˜í™˜  ################

def get_answer(question: str) -> str:
      csv_extraction_response = get_csv_extraction(question)
      if csv_extraction_response["param"] != "none":
            params = csv_extraction_response["param"].split(';')

            params_schema = dict()
            each_param = dict()

            for idx in params:
               each_param[idx] = {"type": "string"}

            params_schema["properties"] = each_param

            # params_llm=ChatOpenAI(temperature=0.0, model_name="gpt-4")
            params_chain = create_extraction_chain(params_schema, base_llm)

            params_response = params_chain.run(question)
            with st.expander('Params Extraction History'): 
               st.info(params_response)
               
            params_data = "?"
            for key, value in params_response[0].items():
                 params_data = params_data + key + "=" + value
                 if value == '':
                     st.warning(key + " ê°’ì— ëŒ€í•œ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. í•„ìš”í•œ ì •ë³´ì™€ í•¨ê»˜ ì§ˆë¬¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
                     return "No data"
            
            ## HTTP GET ìš”ì²­í•˜ì—¬ ë°ì´í„° ë°›ì€ í›„ GPTì— ì§ˆë¬¸ í›„ ëŒ€ë‹µ ë°˜í™˜
            answer = get_request_get_answer(question, csv_extraction_response["url"], params_data, True)
            return answer
      else:
            answer= get_request_get_answer(question, csv_extraction_response["url"], None)
            return answer

######################## ìœ ì € ì§ˆë¬¸ì—ì„œ í•„ìš” param ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ  ########################


####################################### ì§ˆë¬¸ ì‹œì‘ #####################################

# í™”ë©´ì˜ í…ìŠ¤íŠ¸ í•„ë“œì— ë‚´ìš©ì´ ìˆì„ ê²½ìš° ì‘ë™
if question: 
   answer = get_answer(question)

   if answer != "No data":
      with st.expander('Conversation History'): 
         st.info(request_memory.buffer)

   st.write(answer)

####################################### ì§ˆë¬¸ ì™„ë£Œ #####################################
